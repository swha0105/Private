원론적인 decision tree는 regression 에 약함


bias vs variance

bias (목표물에 얼마나 잘맞냐.. low 일수록 목표물에 가까움)

variance (총알끼리의 variance)



high bias => 모델이 멍청한거.. 분석력이 떨어짐 (underfitting) (high train, test error)
high variance => 모델이 너무 잘맞출때.. (overfitting) (low train, high test error)


보편적인 feature추출에 집중.. To prevent overfitting

validation error가 올라간다고 바로 끄면 안됨 (2차 곡선이 아닐수도..)



Ensemble: 여러 모델을 이용하여 결과를 예측하고, 이를 결합하는 것을 Ensemble 


앙상블 하는 방법도 여러가지있음  

bagging (Bootstrap Aggregating) (앙상블 방법) & Ensemble 

bagging + decision tree = random forest